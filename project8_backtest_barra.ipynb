{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 8: Backtesting\n",
    "\n",
    "In this project, we will build a fairly realistic backtester that uses the Barra data. The backtester will perform portfolio optimization that includes **transaction costs**, and we'll implement it with computational efficiency in mind, to allow for a reasonably fast backtest. We'll also use performance attribution to identify the major drivers of our portfolio's profit-and-loss (PnL). We will have the option to modify and customize the backtest as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting alphalens==0.3.2\n",
      "  Using cached alphalens-0.3.2.tar.gz (18.9 MB)\n",
      "Collecting colour==0.1.5\n",
      "  Using cached colour-0.1.5-py2.py3-none-any.whl (23 kB)\n",
      "Collecting cvxpy==1.0.3\n",
      "  Using cached cvxpy-1.0.3.tar.gz (880 kB)\n",
      "Requirement already satisfied: cycler==0.10.0 in /Users/jiadaihe/.pyenv/versions/3.8.6/envs/learn_tensor/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.10.0)\n",
      "Collecting numpy==1.13.3\n",
      "  Downloading numpy-1.13.3.zip (5.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.0 MB 558 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.18.1\n",
      "  Using cached pandas-0.18.1.tar.gz (7.3 MB)\n",
      "Collecting plotly==2.2.3\n",
      "  Using cached plotly-2.2.3.tar.gz (1.1 MB)\n",
      "Collecting pyparsing==2.2.0\n",
      "  Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Collecting python-dateutil==2.6.1\n",
      "  Using cached python_dateutil-2.6.1-py2.py3-none-any.whl (194 kB)\n",
      "Collecting pytz==2017.3\n",
      "  Using cached pytz-2017.3-py2.py3-none-any.whl (511 kB)\n",
      "Collecting requests==2.18.4\n",
      "  Using cached requests-2.18.4-py2.py3-none-any.whl (88 kB)\n",
      "Collecting scipy==1.0.0\n",
      "  Using cached scipy-1.0.0.tar.gz (15.2 MB)\n",
      "Collecting scikit-learn==0.19.1\n",
      "  Using cached scikit-learn-0.19.1.tar.gz (9.5 MB)\n",
      "Collecting six==1.11.0\n",
      "  Using cached six-1.11.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tables==3.3.0\n",
      "  Using cached tables-3.3.0.tar.gz (7.0 MB)\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/jiadaihe/.pyenv/versions/3.8.6/envs/learn_tensor/bin/python3.8 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/yh/pc7jtwg54mvchh6jp4l92jx40000gn/T/pip-install-t1yha32r/tables/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/yh/pc7jtwg54mvchh6jp4l92jx40000gn/T/pip-install-t1yha32r/tables/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /private/var/folders/yh/pc7jtwg54mvchh6jp4l92jx40000gn/T/pip-pip-egg-info-1mjaxhpr\n",
      "         cwd: /private/var/folders/yh/pc7jtwg54mvchh6jp4l92jx40000gn/T/pip-install-t1yha32r/tables/\n",
      "    Complete output (9 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/private/var/folders/yh/pc7jtwg54mvchh6jp4l92jx40000gn/T/pip-install-t1yha32r/tables/setup.py\", line 569, in <module>\n",
      "        hdf5_version = get_hdf5_version(hdf5_header)\n",
      "      File \"/private/var/folders/yh/pc7jtwg54mvchh6jp4l92jx40000gn/T/pip-install-t1yha32r/tables/setup.py\", line 341, in get_hdf5_version\n",
      "        major_version = int(re.split(\"\\s*\", line)[2])\n",
      "    ValueError: invalid literal for int() with base 10: 'd'\n",
      "    * Using Python 3.8.6 (default, Oct 20 2020, 22:26:44)\n",
      "    * USE_PKGCONFIG: True\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Users/jiadaihe/.pyenv/versions/3.8.6/envs/learn_tensor/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import patsy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import median\n",
    "from scipy.stats import gaussian_kde\n",
    "from statsmodels.formula.api import ols\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We’ll be using the Barra dataset to get factors that can be used to predict risk. Loading and parsing the raw Barra data can be a very slow process that can significantly slow down your backtesting. For this reason, it's important to pre-process the data beforehand. Here, the Barra data has already been pre-processed and saved into pickle files. \n",
    "\n",
    "In the code below, we start by loading `2004` factor data from the `pandas-frames.2004.pickle` file. We also load the `2003` and `2004` covariance data from the `covaraince.2003.pickle`  and `covaraince.2004.pickle` files. The data range for the backtest can be customized. It is recommended starting with two or three years of factor data. Remember that the covariance data should include all the years that you choose for the factor data, and also one year earlier. For example, in the code below we are using  `2004` factor data, therefore, we must include `2004` in our covariance data, but also the previous year, `2003`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "barra_dir = '../../data/project_8_barra/'\n",
    "\n",
    "data = {}\n",
    "for year in [2004]:\n",
    "    fil = barra_dir + \"pandas-frames.\" + str(year) + \".pickle\"\n",
    "    data.update(pickle.load( open( fil, \"rb\" ) ))\n",
    "    \n",
    "covariance = {}\n",
    "for year in [2003, 2004]:\n",
    "    fil = barra_dir + \"covariance.\" + str(year) + \".pickle\"\n",
    "    covariance.update(pickle.load( open(fil, \"rb\" ) ))\n",
    "    \n",
    "daily_return = {}\n",
    "for year in [2004, 2005]:\n",
    "    fil = barra_dir + \"price.\" + str(year) + \".pickle\"\n",
    "    daily_return.update(pickle.load( open(fil, \"rb\" ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift Daily Returns Data\n",
    "In the cell below, we want to incorporate a realistic time delay that exists in live trading, we’ll use a two day delay for the `daily_return` data. That means the `daily_return` should be two days after the data in `data` and `cov_data`. Combine `daily_return` and `data` together in a dict called `frames`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Barrid</th>\n",
       "      <th>USFASTD_1DREVRSL</th>\n",
       "      <th>USFASTD_AERODEF</th>\n",
       "      <th>USFASTD_AIRLINES</th>\n",
       "      <th>USFASTD_ALUMSTEL</th>\n",
       "      <th>USFASTD_APPAREL</th>\n",
       "      <th>USFASTD_AUTO</th>\n",
       "      <th>USFASTD_BANKS</th>\n",
       "      <th>USFASTD_BETA</th>\n",
       "      <th>USFASTD_BEVTOB</th>\n",
       "      <th>...</th>\n",
       "      <th>ADTCA_30</th>\n",
       "      <th>IssuerMarketCap</th>\n",
       "      <th>Yield</th>\n",
       "      <th>TotalRisk</th>\n",
       "      <th>SpecRisk</th>\n",
       "      <th>HistBeta</th>\n",
       "      <th>PredBeta</th>\n",
       "      <th>DataDate</th>\n",
       "      <th>DlyReturn</th>\n",
       "      <th>DlyReturnDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USA0001</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.592728e+10</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>15.692850</td>\n",
       "      <td>10.050981</td>\n",
       "      <td>-0.000188</td>\n",
       "      <td>0.159701</td>\n",
       "      <td>20040106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20040108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USA0011</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.029930e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.050196</td>\n",
       "      <td>12.874902</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.133397</td>\n",
       "      <td>20040106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20040108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USA0031</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.251836e+10</td>\n",
       "      <td>2.103004</td>\n",
       "      <td>24.037181</td>\n",
       "      <td>19.772275</td>\n",
       "      <td>0.050603</td>\n",
       "      <td>0.210419</td>\n",
       "      <td>20040106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20040108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USA0062</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.918165e+10</td>\n",
       "      <td>2.243494</td>\n",
       "      <td>25.280406</td>\n",
       "      <td>22.709825</td>\n",
       "      <td>0.074781</td>\n",
       "      <td>0.372498</td>\n",
       "      <td>20040106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20040108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USA00E2</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.784320e+10</td>\n",
       "      <td>2.167256</td>\n",
       "      <td>27.885397</td>\n",
       "      <td>23.513232</td>\n",
       "      <td>0.094615</td>\n",
       "      <td>0.410219</td>\n",
       "      <td>20040106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20040108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Barrid  USFASTD_1DREVRSL  USFASTD_AERODEF  USFASTD_AIRLINES  \\\n",
       "0  USA0001            -0.481            0.000               0.0   \n",
       "1  USA0011            -0.595            0.000               0.0   \n",
       "2  USA0031            -0.109            0.000               0.0   \n",
       "3  USA0062             0.163            0.431               0.0   \n",
       "4  USA00E2             0.064            0.000               0.0   \n",
       "\n",
       "   USFASTD_ALUMSTEL  USFASTD_APPAREL  USFASTD_AUTO  USFASTD_BANKS  \\\n",
       "0               0.0              0.0           0.0            0.0   \n",
       "1               0.0              0.0           0.0            0.0   \n",
       "2               0.0              0.0           0.0            0.0   \n",
       "3               0.0              0.0           0.0            0.0   \n",
       "4               0.0              0.0           0.0            0.0   \n",
       "\n",
       "   USFASTD_BETA  USFASTD_BEVTOB  ...  ADTCA_30  IssuerMarketCap     Yield  \\\n",
       "0        -2.158             0.0  ...       NaN     5.592728e+10  0.188679   \n",
       "1        -2.158             0.0  ...       NaN     6.029930e+09  0.000000   \n",
       "2        -2.049             0.0  ...       NaN     7.251836e+10  2.103004   \n",
       "3        -1.997             0.0  ...       NaN     2.918165e+10  2.243494   \n",
       "4        -1.955             0.0  ...       NaN     5.784320e+10  2.167256   \n",
       "\n",
       "   TotalRisk   SpecRisk  HistBeta  PredBeta  DataDate  DlyReturn  \\\n",
       "0  15.692850  10.050981 -0.000188  0.159701  20040106        0.0   \n",
       "1  19.050196  12.874902  0.000017  0.133397  20040106        0.0   \n",
       "2  24.037181  19.772275  0.050603  0.210419  20040106        0.0   \n",
       "3  25.280406  22.709825  0.074781  0.372498  20040106        0.0   \n",
       "4  27.885397  23.513232  0.094615  0.410219  20040106        0.0   \n",
       "\n",
       "   DlyReturnDate  \n",
       "0       20040108  \n",
       "1       20040108  \n",
       "2       20040108  \n",
       "3       20040108  \n",
       "4       20040108  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames ={}\n",
    "dlyreturn_n_days_delay = 2\n",
    "\n",
    "backtest_dates = sorted(data.keys())\n",
    "dlyreturn_dates = sorted(daily_return.keys())[dlyreturn_n_days_delay : len(backtest_dates)+dlyreturn_n_days_delay]\n",
    "\n",
    "for data_date, price_date in zip(backtest_dates, dlyreturn_dates):\n",
    "    frames[price_date] = data[data_date].merge(daily_return[price_date], on='Barrid')\n",
    "    frames[price_date]['DlyReturnDate'] = pd.Series([price_date]*len(frames[price_date]))\n",
    "    \n",
    "frames['20040108'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USFASTD_1DREVRSL    0.000497\n",
       "USFASTD_AERODEF     0.006654\n",
       "USFASTD_AIRLINES   -0.024867\n",
       "USFASTD_ALUMSTEL   -0.006506\n",
       "USFASTD_APPAREL    -0.002412\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facret = {} # factor returns\n",
    "\n",
    "for date in dlyreturn_dates:\n",
    "    facret[date] = estimate_factor_returns(frames[date])\n",
    "\n",
    "facret['20040108'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dates = sorted(list(map(lambda date: pd.to_datetime(date, format='%Y%m%d'), frames.keys())))\n",
    "alpha_factors = [\"USFASTD_1DREVRSL\", \"USFASTD_EARNYILD\", \"USFASTD_VALUE\", \"USFASTD_SENTMT\"]\n",
    "risk_aversion = 1.0e-6\n",
    "previous_holdings = pd.DataFrame(data = {\"Barrid\" : [\"USA0001\"], \"h.opt.previous\" : np.array(0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Exposures and Factor Returns\n",
    "\n",
    "$r_{i,t} = \\sum_{j=1}^{k} (\\beta_{i,j,t-2} \\times f_{j,t})$  \n",
    "where $i=1...N$ (N assets),   \n",
    "and $j=1...k$ (k factors).\n",
    "\n",
    "where $r_{i,t}$ is the return, $\\beta_{i,j,t-2}$ is the factor exposure, and $f_{j,t}$ is the factor return. Since we get the factor exposures from the Barra data, and we know the returns, we use OLS to estimate the factor returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wins(x,a,b):\n",
    "    \"\"\"Winsorize, avoid extremely positive or negative values in our data.\"\"\"\n",
    "    return np.where(x <= a,a, np.where(x >= b, b, x))\n",
    "\n",
    "def get_formula(factors, Y):\n",
    "    L = [\"0\"]\n",
    "    L.extend(factors)\n",
    "    return Y + \" ~ \" + \" + \".join(L)\n",
    "\n",
    "def factors_from_names(n):\n",
    "    return list(filter(lambda x: \"USFASTD_\" in x, n))\n",
    "\n",
    "def estimate_factor_returns(df): \n",
    "    ## build universe based on filters \n",
    "    estu = df.loc[df.IssuerMarketCap > 1e9].copy(deep=True)\n",
    "  \n",
    "    ## winsorize returns for fitting \n",
    "    estu['DlyReturn'] = wins(estu['DlyReturn'], -0.25, 0.25)\n",
    "  \n",
    "    all_factors = factors_from_names(list(df))\n",
    "    form = get_formula(all_factors, \"DlyReturn\")\n",
    "    model = ols(form, data=estu)\n",
    "    results = model.fit()\n",
    "    return results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def clean_nas(df): \n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    for numeric_column in numeric_columns: \n",
    "        df[numeric_column] = np.nan_to_num(df[numeric_column])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def setdiff(temp1, temp2): \n",
    "    s = set(temp2)\n",
    "    temp3 = [x for x in temp1 if x not in s]\n",
    "    return temp3\n",
    "\n",
    "def model_matrix(formula, data): \n",
    "    outcome, predictors = patsy.dmatrices(formula, data)\n",
    "    return predictors\n",
    "\n",
    "def get_universe(df):\n",
    "    return df.loc[(df['IssuerMarketCap'] > 1e9) | (df['h.opt.previous'] > 0)].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colnames(B):\n",
    "    if type(B) == patsy.design_info.DesignMatrix: \n",
    "        return B.design_info.column_names\n",
    "    if type(B) == pd.core.frame.DataFrame: \n",
    "        return B.columns.tolist()\n",
    "    return None\n",
    "\n",
    "def diagonal_factor_cov(date, B):\n",
    "    \"\"\"\n",
    "    Create the factor covariance matrix\n",
    "    B : Matrix of Risk Factors\n",
    "    \"\"\"\n",
    "    cov = covariance[date]\n",
    "    factor_cov = pd.DataFrame(0, index=colnames(B), columns=colnames(B))\n",
    "    for f1 in colnames(B):\n",
    "        for f2 in colnames(B):\n",
    "            try:\n",
    "                value = cov.loc[(cov.Factor1==f1) & (cov.Factor2==f2),\"VarCovar\"].iloc[0]\n",
    "            except:\n",
    "                value = 0\n",
    "            factor_cov.loc[f1,f2] = value\n",
    "    return factor_cov.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lambda(universe, composite_volume_column = 'ADTCA_30'):\n",
    "    universe.loc[np.isnan(universe[composite_volume_column]), composite_volume_column] = 1.0e4\n",
    "    universe.loc[universe[composite_volume_column] == 0, composite_volume_column] = 1.0e4 \n",
    "    adv = universe[composite_volume_column]\n",
    "    return 0.1 / adv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha combination\n",
    "Now that you have the matrix containing the alpha factors we will combine them by adding its rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_B_alpha(alpha_factors, universe):\n",
    "    outcome, predictors = patsy.dmatrices(get_formula(alpha_factors, \"SpecRisk\"), universe)\n",
    "    return predictors\n",
    "\n",
    "def get_alpha_vec(B_alpha):\n",
    "    return np.sum(B_alpha, axis=1) * 1e-4\n",
    "\n",
    "def get_alpha_vec_weighted(B_alpha, weights):\n",
    "    \"\"\"       \n",
    "    B_alpha : Matrix of Alpha Factors, N * # alpha factors\n",
    "    weights: vector, # alpha factors * 1\n",
    "    Return an alpha vecrtor, N*1\n",
    "    \"\"\"\n",
    "    return np.matmul(B_alpha, weights) * 1e-4\n",
    "\n",
    "from bisect import bisect_left \n",
    "\n",
    "def alpha_weights(alpha_factors, date, window_length=15):\n",
    "    '''Return a weight vector, # alpha factors * 1'''\n",
    "    today = bisect_left(my_dates, pd.to_datetime(date, format='%Y%m%d')) \n",
    "    if today <= window_length:\n",
    "        return np.array([1/len(alpha_factors)] * len(alpha_factors))\n",
    "    \n",
    "    start = today - dlyreturn_n_days_delay - window_length if today-dlyreturn_n_days_delay-window_length > 0 else 0\n",
    "    alpha_returns = []\n",
    "    for i in range(start, today - dlyreturn_n_days_delay + 1):\n",
    "        dt = my_dates[i].strftime('%Y%m%d')\n",
    "        results = facret[dt]\n",
    "        alpha_returns.append(results[alpha_factors])\n",
    "    \n",
    "    sharps = np.mean(alpha_returns, axis=0)/np.std(alpha_returns, axis=0)\n",
    "    sharps = np.nan_to_num(sharps)\n",
    "    sharps = np.maximum(sharps, 0)\n",
    "    return np.array(sharps/np.sum(sharps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_func(h0, risk_aversion, Q, specVar, alpha_vec, Lambda): \n",
    "    '''h0: vector, N*1\n",
    "       risk_aversion: scalar\n",
    "       Q: matrix, K*N\n",
    "       specVar: vector, N*1\n",
    "       alpha_vec: vector, N*1\n",
    "       Lambda: vector, N*1\n",
    "    '''\n",
    "    def obj_func(h):\n",
    "        f = 0.5 * risk_aversion * np.sum(np.matmul(Q, h)**2) + \\\n",
    "            0.5 * risk_aversion * np.dot(h**2, specVar) - \\\n",
    "            np.dot(alpha_vec, h) + np.dot((h-h0)**2, Lambda)\n",
    "        return np.asarray(f)\n",
    "    \n",
    "    return obj_func\n",
    "\n",
    "def get_grad_func(h0, risk_aversion, Q, QT, specVar, alpha_vec, Lambda):\n",
    "    def grad_func(h):\n",
    "        f_prime = risk_aversion * np.matmul(QT, (np.matmul(Q, h))) + \\\n",
    "                  risk_aversion * h * specVar - \\\n",
    "                  alpha_vec + 2 * (h-h0) * Lambda\n",
    "        return np.asarray(f_prime)\n",
    "    \n",
    "    return grad_func\n",
    "\n",
    "def get_h_star(risk_aversion, Q, QT, specVar, alpha_vec, h0, Lambda):\n",
    "    \"\"\"\n",
    "    Optimize the objective function\n",
    "    Return Numpy ndarray -- optimized holdings\n",
    "    \"\"\"\n",
    "    obj_func = get_obj_func(h0, risk_aversion, Q, specVar, alpha_vec, Lambda)\n",
    "    grad_func = get_grad_func(h0, risk_aversion, Q, QT, specVar, alpha_vec, Lambda)\n",
    "    optimizer_result = scipy.optimize.fmin_l_bfgs_b(obj_func, h0, fprime=grad_func)\n",
    "    return optimizer_result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Exposures, Alpha Exposures\n",
    "\n",
    "We can also use `h_star` to calculate our portfolio's risk and alpha exposures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_risk_exposures(B, BT, h_star):\n",
    "    return pd.Series(np.matmul(BT, h_star), index = colnames(B))\n",
    "\n",
    "def get_portfolio_alpha_exposure(B_alpha, h_star):\n",
    "    return pd.Series(np.matmul(B_alpha.transpose(), h_star), index = colnames(B_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction Costs \n",
    "\n",
    "We can also use `h_star` to calculate our total transaction costs:\n",
    "$$\n",
    "\\mbox{tcost} = \\sum_i^{N} \\lambda_{i} (h_{i,t} - h_{i,t-1})^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_transaction_costs(h0, h_star, Lambda):\n",
    "    return np.sum( (h_star - h0)**2 * Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(\\mathbf{h}) = \\frac{1}{2}\\kappa \\mathbf{h}_t^T\\textbf{BFB}^T\\mathbf{h}_t + \\frac{1}{2} \\kappa \\mathbf{h}_t^T \\mathbf{S} \\mathbf{h}_t - \\mathbf{\\alpha}^T \\mathbf{h}_t + (\\mathbf{h}_{t} - \\mathbf{h}_{t-1})^T \\mathbf{\\Lambda} (\\mathbf{h}_{t} - \\mathbf{h}_{t-1})\n",
    "$$\n",
    "where $\\textbf{BFB}^T=\\textbf{Q}^T\\textbf{Q}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_optimal_portfolio(df, previous, risk_aversion, weight_alpha=False):\n",
    "    df = df.merge(previous, how = 'left', on = 'Barrid') # apprend previous holdings to the dataframe, column name is h.opt.previous\n",
    "    df = clean_nas(df) # replace NaN with zero and infinity with large finite numbers\n",
    "    df.loc[df['SpecRisk'] == 0]['SpecRisk'] = median(df['SpecRisk']) \n",
    "  \n",
    "    universe = get_universe(df) # market cap > 1 billion or previously holding it\n",
    "    date = str(int(universe['DataDate'][1])) # the date of factor exposures (betas) in Barra data \n",
    "\n",
    "    all_factors = factors_from_names(list(universe))\n",
    "    risk_factors = setdiff(all_factors, alpha_factors)\n",
    "  \n",
    "    h0 = universe['h.opt.previous']\n",
    "  \n",
    "    B = model_matrix(get_formula(risk_factors, \"SpecRisk\"), universe) # B at data_date\n",
    "    BT = B.transpose()\n",
    "  \n",
    "    specVar = (0.01 * universe['SpecRisk']) ** 2 # specific variance at data_date\n",
    "    Fvar = diagonal_factor_cov(date, B) # risk factor covariance matrix at data_date\n",
    "    \n",
    "    Lambda = get_lambda(universe)\n",
    "    B_alpha = get_B_alpha(alpha_factors, universe)\n",
    "    alpha_vec = get_alpha_vec(B_alpha)\n",
    "    \n",
    "    if weight_alpha:\n",
    "        weights = alpha_weights(alpha_factors, date)\n",
    "        alpha_vec = get_alpha_vec_weighted(B_alpha, weights)\n",
    "    \n",
    "    Q = np.matmul(scipy.linalg.sqrtm(Fvar), BT)\n",
    "    QT = Q.transpose()\n",
    "    \n",
    "    h_star = get_h_star(risk_aversion, Q, QT, specVar, alpha_vec, h0, Lambda)\n",
    "    opt_portfolio = pd.DataFrame(data = {\"Barrid\" : universe['Barrid'], \"h.opt\" : h_star})\n",
    "    \n",
    "    risk_exposures = get_risk_exposures(B, BT, h_star)\n",
    "    portfolio_alpha_exposure = get_portfolio_alpha_exposure(B_alpha, h_star)\n",
    "    total_transaction_costs = get_total_transaction_costs(h0, h_star, Lambda)\n",
    "  \n",
    "    return {\n",
    "        \"opt.portfolio\" : opt_portfolio, \n",
    "        \"risk.exposures\" : risk_exposures, \n",
    "        \"alpha.exposures\" : portfolio_alpha_exposure,\n",
    "        \"total.cost\" : total_transaction_costs}\n",
    "\n",
    "def build_tradelist(prev_holdings, opt_result):\n",
    "    tmp = prev_holdings.merge(opt_result['opt.portfolio'], how='outer', on = 'Barrid')\n",
    "    tmp['h.opt.previous'] = np.nan_to_num(tmp['h.opt.previous'])\n",
    "    tmp['h.opt'] = np.nan_to_num(tmp['h.opt'])\n",
    "    return tmp\n",
    "\n",
    "def convert_to_previous(result): \n",
    "    prev = result['opt.portfolio']\n",
    "    prev = prev.rename(index=str, columns={\"h.opt\": \"h.opt.previous\"}, copy=True, inplace=False)\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing Portfolio:   0%|          | 0/5 [00:00<?, ?day/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.5200e-05 -6.9200e-05 -9.0750e-06 ...  4.7500e-07  4.1975e-05\n",
      "  9.8250e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing Portfolio:  20%|██        | 1/5 [00:38<02:32, 38.20s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.1000e-06 -8.2150e-05 -3.2425e-05 ...  3.4525e-05 -8.7500e-06\n",
      "  1.4100e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing Portfolio:  40%|████      | 2/5 [01:15<01:53, 37.88s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.8750e-05 -9.2250e-05 -2.2750e-05 ...  9.9750e-06  3.4475e-05\n",
      "  1.3400e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing Portfolio:  60%|██████    | 3/5 [01:52<01:14, 37.38s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.81462569e-05 -1.20896907e-04 -7.35331918e-06 ... -6.29743424e-05\n",
      "  1.68873398e-04 -6.37774347e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing Portfolio:  80%|████████  | 4/5 [02:30<00:37, 37.72s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.26022567e-05 -1.08979133e-04 -3.54104118e-05 ... -1.63019099e-05\n",
      "  6.44950574e-05 -5.13865069e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing Portfolio: 100%|██████████| 5/5 [03:07<00:00, 37.48s/day]\n"
     ]
    }
   ],
   "source": [
    "trades = {}\n",
    "port = {}\n",
    "\n",
    "for dt in tqdm(my_dates[:5], desc='Optimizing Portfolio', unit='day'):\n",
    "    date = dt.strftime('%Y%m%d')\n",
    "\n",
    "    result = form_optimal_portfolio(frames[date], previous_holdings, risk_aversion)\n",
    "    trades[date] = build_tradelist(previous_holdings, result)\n",
    "    port[date] = result\n",
    "    previous_holdings = convert_to_previous(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profit-and-Loss (PnL) attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assumes v, w are pandas Series \n",
    "def partial_dot_product(v, w):\n",
    "    common = v.index.intersection(w.index)\n",
    "    return np.sum(v[common] * w[common])\n",
    "\n",
    "def build_pnl_attribution(): \n",
    "\n",
    "    df = pd.DataFrame(index = my_dates)\n",
    "    \n",
    "    for dt in my_dates[:2]:\n",
    "        date = dt.strftime('%Y%m%d')\n",
    "        \n",
    "        p = port[date]\n",
    "        fr = facret[date] # factor returns\n",
    "        \n",
    "        mf = p['opt.portfolio'].merge(frames[date], how = 'left', on = \"Barrid\")\n",
    "        \n",
    "        mf['DlyReturn'] = wins(mf['DlyReturn'], -0.5, 0.5)\n",
    "        df.at[dt,\"daily.pnl\"] = np.sum(mf['h.opt'] * mf['DlyReturn'])\n",
    "\n",
    "        # TODO: Implement\n",
    "        df.at[dt,\"attribution.alpha.pnl\"] = partial_dot_product(fr, p[\"alpha.exposures\"])\n",
    "        df.at[dt,\"attribution.risk.pnl\"] = partial_dot_product(fr, p[\"risk.exposures\"] )\n",
    "        df.at[dt,\"attribution.cost\"] = p[\"total.cost\"]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daily.pnl</th>\n",
       "      <th>attribution.alpha.pnl</th>\n",
       "      <th>attribution.risk.pnl</th>\n",
       "      <th>attribution.cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-01-06</th>\n",
       "      <td>238.407678</td>\n",
       "      <td>1489.421637</td>\n",
       "      <td>0.006977</td>\n",
       "      <td>6.050174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-07</th>\n",
       "      <td>-600.189818</td>\n",
       "      <td>308.273480</td>\n",
       "      <td>0.021572</td>\n",
       "      <td>3.561889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-11-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-11-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-11-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-11-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             daily.pnl  attribution.alpha.pnl  attribution.risk.pnl  \\\n",
       "2004-01-06  238.407678            1489.421637              0.006977   \n",
       "2004-01-07 -600.189818             308.273480              0.021572   \n",
       "2004-01-08         NaN                    NaN                   NaN   \n",
       "2004-01-09         NaN                    NaN                   NaN   \n",
       "2004-01-12         NaN                    NaN                   NaN   \n",
       "2004-01-13         NaN                    NaN                   NaN   \n",
       "2004-01-14         NaN                    NaN                   NaN   \n",
       "2004-01-15         NaN                    NaN                   NaN   \n",
       "2004-01-16         NaN                    NaN                   NaN   \n",
       "2004-01-20         NaN                    NaN                   NaN   \n",
       "2004-01-21         NaN                    NaN                   NaN   \n",
       "2004-01-22         NaN                    NaN                   NaN   \n",
       "2004-01-23         NaN                    NaN                   NaN   \n",
       "2004-01-26         NaN                    NaN                   NaN   \n",
       "2004-01-27         NaN                    NaN                   NaN   \n",
       "2004-01-28         NaN                    NaN                   NaN   \n",
       "2004-01-29         NaN                    NaN                   NaN   \n",
       "2004-01-30         NaN                    NaN                   NaN   \n",
       "2004-02-02         NaN                    NaN                   NaN   \n",
       "2004-02-03         NaN                    NaN                   NaN   \n",
       "2004-02-04         NaN                    NaN                   NaN   \n",
       "2004-02-05         NaN                    NaN                   NaN   \n",
       "2004-02-06         NaN                    NaN                   NaN   \n",
       "2004-02-09         NaN                    NaN                   NaN   \n",
       "2004-02-10         NaN                    NaN                   NaN   \n",
       "2004-02-11         NaN                    NaN                   NaN   \n",
       "2004-02-12         NaN                    NaN                   NaN   \n",
       "2004-02-13         NaN                    NaN                   NaN   \n",
       "2004-02-17         NaN                    NaN                   NaN   \n",
       "2004-02-18         NaN                    NaN                   NaN   \n",
       "...                ...                    ...                   ...   \n",
       "2004-11-22         NaN                    NaN                   NaN   \n",
       "2004-11-23         NaN                    NaN                   NaN   \n",
       "2004-11-24         NaN                    NaN                   NaN   \n",
       "2004-11-26         NaN                    NaN                   NaN   \n",
       "2004-11-29         NaN                    NaN                   NaN   \n",
       "2004-11-30         NaN                    NaN                   NaN   \n",
       "2004-12-01         NaN                    NaN                   NaN   \n",
       "2004-12-02         NaN                    NaN                   NaN   \n",
       "2004-12-03         NaN                    NaN                   NaN   \n",
       "2004-12-06         NaN                    NaN                   NaN   \n",
       "2004-12-07         NaN                    NaN                   NaN   \n",
       "2004-12-08         NaN                    NaN                   NaN   \n",
       "2004-12-09         NaN                    NaN                   NaN   \n",
       "2004-12-10         NaN                    NaN                   NaN   \n",
       "2004-12-13         NaN                    NaN                   NaN   \n",
       "2004-12-14         NaN                    NaN                   NaN   \n",
       "2004-12-15         NaN                    NaN                   NaN   \n",
       "2004-12-16         NaN                    NaN                   NaN   \n",
       "2004-12-17         NaN                    NaN                   NaN   \n",
       "2004-12-20         NaN                    NaN                   NaN   \n",
       "2004-12-21         NaN                    NaN                   NaN   \n",
       "2004-12-22         NaN                    NaN                   NaN   \n",
       "2004-12-23         NaN                    NaN                   NaN   \n",
       "2004-12-27         NaN                    NaN                   NaN   \n",
       "2004-12-28         NaN                    NaN                   NaN   \n",
       "2004-12-29         NaN                    NaN                   NaN   \n",
       "2004-12-30         NaN                    NaN                   NaN   \n",
       "2004-12-31         NaN                    NaN                   NaN   \n",
       "2005-01-03         NaN                    NaN                   NaN   \n",
       "2005-01-04         NaN                    NaN                   NaN   \n",
       "\n",
       "            attribution.cost  \n",
       "2004-01-06          6.050174  \n",
       "2004-01-07          3.561889  \n",
       "2004-01-08               NaN  \n",
       "2004-01-09               NaN  \n",
       "2004-01-12               NaN  \n",
       "2004-01-13               NaN  \n",
       "2004-01-14               NaN  \n",
       "2004-01-15               NaN  \n",
       "2004-01-16               NaN  \n",
       "2004-01-20               NaN  \n",
       "2004-01-21               NaN  \n",
       "2004-01-22               NaN  \n",
       "2004-01-23               NaN  \n",
       "2004-01-26               NaN  \n",
       "2004-01-27               NaN  \n",
       "2004-01-28               NaN  \n",
       "2004-01-29               NaN  \n",
       "2004-01-30               NaN  \n",
       "2004-02-02               NaN  \n",
       "2004-02-03               NaN  \n",
       "2004-02-04               NaN  \n",
       "2004-02-05               NaN  \n",
       "2004-02-06               NaN  \n",
       "2004-02-09               NaN  \n",
       "2004-02-10               NaN  \n",
       "2004-02-11               NaN  \n",
       "2004-02-12               NaN  \n",
       "2004-02-13               NaN  \n",
       "2004-02-17               NaN  \n",
       "2004-02-18               NaN  \n",
       "...                      ...  \n",
       "2004-11-22               NaN  \n",
       "2004-11-23               NaN  \n",
       "2004-11-24               NaN  \n",
       "2004-11-26               NaN  \n",
       "2004-11-29               NaN  \n",
       "2004-11-30               NaN  \n",
       "2004-12-01               NaN  \n",
       "2004-12-02               NaN  \n",
       "2004-12-03               NaN  \n",
       "2004-12-06               NaN  \n",
       "2004-12-07               NaN  \n",
       "2004-12-08               NaN  \n",
       "2004-12-09               NaN  \n",
       "2004-12-10               NaN  \n",
       "2004-12-13               NaN  \n",
       "2004-12-14               NaN  \n",
       "2004-12-15               NaN  \n",
       "2004-12-16               NaN  \n",
       "2004-12-17               NaN  \n",
       "2004-12-20               NaN  \n",
       "2004-12-21               NaN  \n",
       "2004-12-22               NaN  \n",
       "2004-12-23               NaN  \n",
       "2004-12-27               NaN  \n",
       "2004-12-28               NaN  \n",
       "2004-12-29               NaN  \n",
       "2004-12-30               NaN  \n",
       "2004-12-31               NaN  \n",
       "2005-01-03               NaN  \n",
       "2005-01-04               NaN  \n",
       "\n",
       "[252 rows x 4 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_pnl_attribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = frames['20040106'].merge(previous_holdings, how = 'left', on = 'Barrid') \n",
    "df = clean_nas(df) # replace NaN with zero and infinity with large finite numbers\n",
    "df.loc[df['SpecRisk'] == 0]['SpecRisk'] = median(df['SpecRisk']) \n",
    "\n",
    "universe = get_universe(df)\n",
    "B_alpha = get_B_alpha(alpha_factors, universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2267, 4)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
